services:
  # ---------------------------------------------------------------------------
  # Analytics Backend (OSS)
  # ---------------------------------------------------------------------------
  core:
    build:
      context: ./core
      dockerfile: Dockerfile.dev
    container_name: seentics-core
    ports:
      - "3002:3002"
    environment:
      - ENVIRONMENT=development
      - PORT=3002
      - DATABASE_URL=postgres://seentics:seentics_pass@seentics-postgres:5432/seentics_analytics?sslmode=disable
      - LOG_LEVEL=info
      - JWT_SECRET=${JWT_SECRET:-your-super-secret-jwt-key--must-be-32-characters}
      - REDIS_URL=redis://:${REDIS_PASSWORD}@seentics-redis:6379
      - REDIS_PASSWORD=${REDIS_PASSWORD}
      - GLOBAL_API_KEY=${GLOBAL_API_KEY}
      - NATS_URL=nats://seentics-nats:4222
      - NATS_SUBJECT_EVENTS=analytics.events
      - CLOUD_ENABLED=false
      - CORS_ALLOWED_ORIGINS=${CORS_ALLOWED_ORIGINS:-http://localhost:3000}
      - CLICKHOUSE_HOST=seentics-clickhouse
      - CLICKHOUSE_PORT=9000
      - CLICKHOUSE_USER=default
      - CLICKHOUSE_PASSWORD=seentics_clickhouse_pass
      - CLICKHOUSE_DB=seentics
      - S3_ENDPOINT=http://minio:9000
      - S3_BUCKET_REPLAYS=seentics-replays
      - AWS_REGION=us-east-1
      - AWS_ACCESS_KEY_ID=minioadmin
      - AWS_SECRET_ACCESS_KEY=minioadmin
    depends_on:
      redis:
        condition: service_healthy
      nats:
        condition: service_healthy
      clickhouse:
        condition: service_healthy
      postgres:
        condition: service_healthy
      minio:
        condition: service_healthy
    volumes:
      - ./core:/app
      - ./core/data:/app/data
    restart: unless-stopped
    networks:
      - seentics-network
    healthcheck:
      test: [ "CMD", "go", "version" ] # Simplified health check if curl is missing
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 5s

  # Infrastructure
  # ---------------------------------------------------------------------------
  redis:
    image: redis:7-alpine
    container_name: seentics-redis
    # No port 6379 needed on host if ALB handles public traffic
    command: [ "redis-server", "--requirepass", "${REDIS_PASSWORD}", "--maxmemory", "256mb", "--maxmemory-policy", "allkeys-lru", "--save", "", "--appendonly", "no" ]
    volumes:
      - seentics-redis-v3:/data
    networks:
      - seentics-network
    restart: unless-stopped
    healthcheck:
      test: [ "CMD", "redis-cli", "-a", "${REDIS_PASSWORD}", "ping" ]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 5s

  postgres:
    image: postgres:15-alpine
    container_name: seentics-postgres
    environment:
      - POSTGRES_USER=seentics
      - POSTGRES_PASSWORD=seentics_pass
      - POSTGRES_DB=seentics_analytics
    volumes:
      - seentics-db-v3:/var/lib/postgresql/data
    networks:
      - seentics-network
    restart: unless-stopped
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U seentics -d seentics_analytics" ]
      interval: 10s
      timeout: 5s
      retries: 5

  nats:
    image: nats:2.10-alpine
    container_name: seentics-nats
    command: -c /etc/nats/nats-server.conf
    networks:
      - seentics-network
    volumes:
      - seentics-nats-v3:/data
      - ./nats-server.conf:/etc/nats/nats-server.conf:ro
    restart: unless-stopped
    healthcheck:
      test: [ "CMD", "wget", "--spider", "-q", "http://localhost:8222/healthz" ]
      interval: 10s
      timeout: 5s
      retries: 3

  clickhouse:
    image: clickhouse/clickhouse-server:latest
    container_name: seentics-clickhouse
    environment:
      - CLICKHOUSE_USER=default
      - CLICKHOUSE_PASSWORD=seentics_clickhouse_pass
      - CLICKHOUSE_DB=seentics
    ports:
      - "8123:8123"
      - "9002:9000"
    volumes:
      - seentics-ch-v3:/var/lib/clickhouse
    networks:
      - seentics-network
    restart: unless-stopped
    healthcheck:
      test: [ "CMD", "wget", "--spider", "-q", "localhost:8123/ping" ]
      interval: 10s
      timeout: 5s
      retries: 3

  minio:
    image: minio/minio:latest
    container_name: seentics-minio
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      - MINIO_ROOT_USER=minioadmin
      - MINIO_ROOT_PASSWORD=minioadmin
    command: server /data --console-address ":9001"
    volumes:
      - seentics-minio-v3:/data
    networks:
      - seentics-network
    restart: unless-stopped
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:9000/minio/health/live" ]
      interval: 10s
      timeout: 5s
      retries: 3

  createbuckets:
    image: minio/mc
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: >
      /bin/sh -c " /usr/bin/mc alias set myminio http://minio:9000 minioadmin minioadmin; /usr/bin/mc mb --ignore-existing myminio/seentics-replays; /usr/bin/mc anonymous set public myminio/seentics-replays; exit 0; "
    networks:
      - seentics-network

  # ---------------------------------------------------------------------------
  # Frontend Web (Next.js)
  # ---------------------------------------------------------------------------
  web:
    build:
      context: ./web
      dockerfile: Dockerfile.dev
    container_name: seentics-web
    ports:
      - "3000:3000"
    environment:
      - NODE_ENV=development
      - NEXT_PUBLIC_API_URL=http://localhost:3002/api/v1
      - API_GATEWAY_URL=http://core:3002
    volumes:
      - ./web:/app
      - /app/node_modules
      - /app/.next
    command: sh -c "mkdir -p .next/cache && rm -rf .next/cache/* && npm install --legacy-peer-deps && npm run dev"
    depends_on:
      core:
        condition: service_healthy
    networks:
      - seentics-network
    restart: unless-stopped

volumes:
  seentics-redis-v3:
  seentics-nats-v3:
  seentics-ch-v3:
  seentics-db-v3:
  seentics-minio-v3:


networks:
  seentics-network:
    name: seentics-network
    driver: bridge
